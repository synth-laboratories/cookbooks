[prompt_learning]
algorithm = "gepa"
task_app_url = "http://127.0.0.1:8120"  # Will be overridden by InProcessTaskApp
task_app_api_key = "${ENVIRONMENT_API_KEY}"
env_file_path = "../../../.env"
results_folder = "results"

[prompt_learning.initial_prompt]
id = "banking77_pattern"
name = "Banking77 Intent Classification Pattern"

[[prompt_learning.initial_prompt.messages]]
role = "system"
pattern = "You are an expert banking assistant that classifies customer queries into banking intents. Given a customer message, respond with exactly one intent label from the provided list using the `banking77_classify` tool."
order = 0

[[prompt_learning.initial_prompt.messages]]
role = "user"
pattern = "Customer Query: {query}\n\nAvailable Intents:\n{available_intents}\n\nClassify this query into one of the above banking intents using the tool call."
order = 1

[prompt_learning.initial_prompt.wildcards]
query = "REQUIRED"
available_intents = "REQUIRED"

[prompt_learning.policy]
inference_mode = "synth_hosted"
model = "gpt-4.1-nano"
provider = "openai"
temperature = 0.0
max_completion_tokens = 64

[prompt_learning.gepa]
env_name = "banking77"
proposer_effort = "MEDIUM"
proposer_output_tokens = "FAST"
dspy_temperature = 1.0
synth_temperature = 1.0
gepa_ai_temperature = 1.0

[prompt_learning.gepa.evaluation]
# Tiny test set: 13 seeds (10 for pareto, 3 for feedback) for quick testing
# Minimum requirements: pareto_set_size >= 10, feedback_count >= 3
train_seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
val_seeds = [200, 201, 202, 203, 204]
validation_pool = "train"
validation_top_k = 2  # Evaluate top 2 candidates

[prompt_learning.gepa.rollout]
budget = 15  # Tiny budget for quick test
max_concurrent = 5
minibatch_size = 3  # Minimum allowed is 3

[prompt_learning.gepa.mutation]
rate = 0.3
llm_model = "gpt-5-mini"
llm_provider = "openai"
llm_inference_url = "https://api.openai.com/v1"
temperature = 1.0
max_tokens = 8192

[prompt_learning.gepa.population]
initial_size = 2  # Start with just 2 candidates
num_generations = 2  # Only 2 generations
children_per_generation = 1  # Just 1 child per generation

[prompt_learning.gepa.archive]
max_size = 3  # Keep only 3 candidates
pareto_set_size = 10  # Minimum pareto set: 10 seeds (required)
min_score_threshold = 0.0
feedback_fraction = 0.231  # 3/13 = 0.231 feedback fraction (3 seeds for feedback, 10 for pareto)

[prompt_learning.gepa.token]
max_limit = 4096
counting_model = "gpt-4"
enforce_limit = false

[prompt_learning.termination_config]
max_cost_usd = 0.50  # Tiny cost limit for quick test
max_trials = 5  # Very small number of trials

[display]
local_backend = true
tui = false
show_curve = true
verbose_summary = true
show_trial_results = true
show_transformations = false
show_validation = true

