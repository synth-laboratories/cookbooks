# GEPA (Genetic Evolution of Prompt Architectures) Configuration
#
# GEPA uses genetic algorithms to evolve prompt structures:
# - Mutation operators (add/remove/modify prompt sections)
# - Crossover (combine best parts of different prompts)
# - Selection (keep top-performing variants)
#
# Best for:
# - Complex prompt structures
# - Multi-component prompts (system + few-shot + chain-of-thought)
# - Exploring diverse prompt mutations

[prompt_learning]
algorithm = "gepa"
task_app_url = "http://localhost:8001"
task_app_id = "banking77"

# Policy: Model for generating responses
[prompt_learning.policy]
model = "openai/gpt-oss-20b"
provider = "groq"
temperature = 0.0
max_completion_tokens = 512
policy_name = "banking77-classifier"

# Initial prompt template
[prompt_learning.initial_prompt]
id = "baseline_v1"
name = "Baseline Classification Prompt"

[[prompt_learning.initial_prompt.messages]]
role = "system"
pattern = "You are a banking assistant that classifies customer intents. Analyze the customer's message and determine their intent category."
order = 0

[[prompt_learning.initial_prompt.messages]]
role = "user"
pattern = "Input: {input}\n\nClassify this input."
order = 1

[prompt_learning.initial_prompt.wildcards]
input = "REQUIRED"

# Environment config (dataset split)
[prompt_learning.env_config]
pool = "train"

# GEPA Algorithm Configuration
[prompt_learning.gepa]
env_name = "banking77"
proposer_type = "dspy"
proposer_effort = "LOW"
proposer_output_tokens = "FAST"

# Rollout budget
[prompt_learning.gepa.rollout]
budget = 100
max_concurrent = 20
minibatch_size = 10

# Evaluation seeds - using range notation for readability
[prompt_learning.gepa.evaluation]
seeds = { start = 0, end = 30 }
validation_seeds = { start = 30, end = 50 }
validation_pool = "validation"
validation_top_k = 3

# Mutation settings
[prompt_learning.gepa.mutation]
rate = 0.3

# Population settings
[prompt_learning.gepa.population]
initial_size = 10
num_generations = 5
children_per_generation = 12
crossover_rate = 0.5
patience_generations = 3

# Archive settings
[prompt_learning.gepa.archive]
size = 40
pareto_set_size = 32

# Token/budget limits
[prompt_learning.gepa.token]
max_spend_usd = 10.0
counting_model = "gpt-4"
