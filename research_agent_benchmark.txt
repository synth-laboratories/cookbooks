# Research Agent Benchmark Specification

## Goal

Create a reproducible evaluation framework for Synth's research agent that:
1. Runs the agent on a set of benchmark tasks
2. Extracts a self-contained reproduction script from the agent's output
3. Runs that script locally to verify results
4. Produces a numerical score for each task
5. Aggregates scores across tasks for overall agent quality measurement

---

## Core Concept: Reproduction Script

Every research agent job MUST produce a `reproduce.py` script that:
- Is completely self-contained (no external dependencies beyond synth-ai + standard libs)
- Downloads the dataset automatically
- Contains the task app inline or as a separate file
- Runs end-to-end with just an API key
- Returns results in a standardized JSON format

### Reproduction Script Template

```python
#!/usr/bin/env python3
"""
Reproduction script for {TASK_NAME} prompt optimization.

Generated by: Synth Research Agent
Job ID: {JOB_ID}
Algorithm: {ALGORITHM}
Timestamp: {TIMESTAMP}

Run with:
    export SYNTH_API_KEY="your-key"
    python reproduce.py

This script will:
1. Download the dataset
2. Start the task app
3. Run the optimization
4. Print results in standardized format
"""

import os
import sys
import json
import asyncio
from pathlib import Path
from datetime import datetime

# ============================================================================
# Configuration (modify for different runs)
# ============================================================================

BACKEND_URL = "https://agent-learning.onrender.com"
TASK_APP_PORT = 8114
EVAL_SEEDS = [0, 1, 2, 3, 4]  # Seeds for final evaluation
DATASET_URL = "{DATASET_URL}"  # Or inline dataset below

# ============================================================================
# Dataset (inline or downloaded)
# ============================================================================

DATASET = [
    # Inline dataset samples if small, or download from URL
    # {"input": "...", "expected": "...", "seed": 0},
]

def load_dataset() -> list:
    """Load or download the dataset."""
    if DATASET:
        return DATASET
    
    # Download from URL
    import urllib.request
    print(f"Downloading dataset from {DATASET_URL}...")
    with urllib.request.urlopen(DATASET_URL) as resp:
        return json.loads(resp.read().decode())

# ============================================================================
# Task App
# ============================================================================

TASK_APP_CODE = '''
{TASK_APP_CODE}
'''

# ============================================================================
# Optimized Prompt (the agent's result)
# ============================================================================

OPTIMIZED_PROMPT = """
{OPTIMIZED_PROMPT}
"""

BASELINE_PROMPT = """
{BASELINE_PROMPT}
"""

# ============================================================================
# Evaluation Logic
# ============================================================================

async def evaluate_prompt(prompt: str, dataset: list, seeds: list[int]) -> dict:
    """Evaluate a prompt on the dataset across multiple seeds."""
    from synth_ai.sdk.task import InProcessTaskApp
    import httpx
    import tempfile
    
    # Write task app to temp file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
        f.write(TASK_APP_CODE)
        task_app_path = f.name
    
    try:
        async with InProcessTaskApp(
            task_app_path=Path(task_app_path),
            port=TASK_APP_PORT,
        ) as task_app:
            results = []
            
            async with httpx.AsyncClient(timeout=60) as client:
                for seed in seeds:
                    # Call rollout endpoint
                    response = await client.post(
                        f"{task_app.url}/rollout",
                        json={
                            "seed": seed,
                            "inference_url": f"{BACKEND_URL}/v1",  # Uses default policy
                            "trace_correlation_id": f"eval_{seed}_{datetime.utcnow().timestamp()}",
                            "prompt_template": prompt,
                        },
                        headers={"Authorization": f"Bearer {os.environ.get('SYNTH_API_KEY', '')}"}
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        results.append({
                            "seed": seed,
                            "score": data.get("metrics", {}).get("mean_return", 0),
                            "success": True,
                        })
                    else:
                        results.append({
                            "seed": seed,
                            "score": 0,
                            "success": False,
                            "error": response.text[:200],
                        })
            
            scores = [r["score"] for r in results if r["success"]]
            return {
                "mean_score": sum(scores) / len(scores) if scores else 0,
                "std_score": (sum((s - sum(scores)/len(scores))**2 for s in scores) / len(scores))**0.5 if len(scores) > 1 else 0,
                "n_success": len(scores),
                "n_total": len(seeds),
                "details": results,
            }
    finally:
        os.unlink(task_app_path)


async def main():
    """Run reproduction evaluation."""
    print("=" * 60)
    print("REPRODUCTION EVALUATION")
    print("=" * 60)
    
    # Check API key
    if not os.environ.get("SYNTH_API_KEY"):
        print("ERROR: SYNTH_API_KEY environment variable not set")
        sys.exit(1)
    
    # Load dataset
    dataset = load_dataset()
    print(f"Loaded {len(dataset)} samples")
    
    # Evaluate baseline
    print("\n--- Evaluating BASELINE prompt ---")
    baseline_results = await evaluate_prompt(BASELINE_PROMPT, dataset, EVAL_SEEDS)
    
    # Evaluate optimized
    print("\n--- Evaluating OPTIMIZED prompt ---")
    optimized_results = await evaluate_prompt(OPTIMIZED_PROMPT, dataset, EVAL_SEEDS)
    
    # Calculate improvement
    improvement = 0
    if baseline_results["mean_score"] > 0:
        improvement = ((optimized_results["mean_score"] - baseline_results["mean_score"]) 
                      / baseline_results["mean_score"]) * 100
    
    # Print results
    print("\n" + "=" * 60)
    print("RESULTS")
    print("=" * 60)
    
    results = {
        "task_name": "{TASK_NAME}",
        "algorithm": "{ALGORITHM}",
        "job_id": "{JOB_ID}",
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "eval_seeds": EVAL_SEEDS,
        "baseline": {
            "mean_score": baseline_results["mean_score"],
            "std_score": baseline_results["std_score"],
            "n_success": baseline_results["n_success"],
        },
        "optimized": {
            "mean_score": optimized_results["mean_score"],
            "std_score": optimized_results["std_score"],
            "n_success": optimized_results["n_success"],
        },
        "improvement_percent": improvement,
        "reproduction_success": True,
    }
    
    print(json.dumps(results, indent=2))
    
    # Write to file
    output_path = Path("reproduction_results.json")
    with open(output_path, "w") as f:
        json.dump(results, f, indent=2)
    print(f"\nResults saved to: {output_path}")
    
    return results


if __name__ == "__main__":
    asyncio.run(main())
```

---

## Scoring System

### Per-Task Score (0-100)

```
task_score = reproduction_weight * reproduction_score 
           + improvement_weight * improvement_score
           + artifacts_weight * artifacts_score

Where:
- reproduction_weight = 0.4 (script runs successfully)
- improvement_weight = 0.4 (optimized > baseline)
- artifacts_weight = 0.2 (all required files present)
```

### Reproduction Score (0-100)
- 0: Script doesn't exist or doesn't run
- 50: Script runs but produces errors
- 75: Script runs, partial results
- 100: Script runs end-to-end, all seeds evaluated

### Improvement Score (0-100)
```python
def improvement_score(baseline, optimized):
    if baseline <= 0:
        return 50  # Can't measure improvement
    
    improvement_pct = (optimized - baseline) / baseline * 100
    
    if improvement_pct <= 0:
        return 0
    elif improvement_pct < 5:
        return 25
    elif improvement_pct < 10:
        return 50
    elif improvement_pct < 20:
        return 75
    else:
        return 100
```

### Artifacts Score (0-100)
- reproduce.py exists: +40
- task_app.py exists: +20
- config.toml exists: +20
- optimization_report.json exists: +20

---

## Benchmark Tasks

### Tier 1: Core Tasks (Start Here)

| Task ID | Dataset | Algorithm | Difficulty | Est. Time |
|---------|---------|-----------|------------|-----------|
| `mipro-banking77` | Banking77 | MIPRO | Easy | 10-15 min |
| `gepa-banking77` | Banking77 | GEPA | Easy | 10-15 min |
| `mipro-iris` | Iris | MIPRO | Easy | 5-10 min |
| `mipro-heartdisease` | Heart Disease | MIPRO | Medium | 10-15 min |

### Tier 2: LangProbe Tasks

| Task ID | Dataset | Algorithm | Difficulty | Est. Time |
|---------|---------|-----------|------------|-----------|
| `mipro-gsm8k` | GSM8K (math) | MIPRO | Hard | 20-30 min |
| `mipro-hover` | Hover (fact checking) | MIPRO | Medium | 15-20 min |
| `mipro-pupa` | Pupa | MIPRO | Medium | 15-20 min |
| `mipro-hotpotqa` | HotpotQA | MIPRO | Hard | 20-30 min |

### Tier 3: Multi-Step Tasks

| Task ID | Dataset | Algorithm | Difficulty | Est. Time |
|---------|---------|-----------|------------|-----------|
| `mipro-banking77-3step` | Banking77 | MIPRO | Hard | 25-35 min |
| `gepa-banking77-3step` | Banking77 | GEPA | Hard | 25-35 min |
| `mipro-hotpotqa-cot` | HotpotQA | MIPRO | Very Hard | 30-45 min |

---

## Task Definitions

### banking77
```yaml
task_id: mipro-banking77
dataset:
  name: banking77
  source: huggingface
  hf_path: "PolyAI/banking77"
  n_train: 1000
  n_eval: 200
  format: jsonl
  download_url: "https://synth-public-data.s3.amazonaws.com/banking77_train.jsonl"

evaluation:
  metric: accuracy
  seeds: [0, 1, 2, 3, 4]
  expected_baseline: 0.50-0.60
  good_improvement: 0.70+
  
task_app:
  type: classification
  input_field: text
  output_field: intent
  n_classes: 77
```

### iris
```yaml
task_id: mipro-iris
dataset:
  name: iris
  source: sklearn
  n_samples: 150
  n_eval: 30
  download_url: "https://synth-public-data.s3.amazonaws.com/iris_train.jsonl"
  
evaluation:
  metric: accuracy
  seeds: [0, 1, 2]
  expected_baseline: 0.85-0.90
  good_improvement: 0.95+
  
task_app:
  type: classification
  input_field: features
  output_field: species
  n_classes: 3
```

### heartdisease
```yaml
task_id: mipro-heartdisease
dataset:
  name: heart_disease
  source: uci
  n_samples: 303
  n_eval: 60
  download_url: "https://synth-public-data.s3.amazonaws.com/heartdisease_train.jsonl"
  
evaluation:
  metric: accuracy
  seeds: [0, 1, 2, 3, 4]
  expected_baseline: 0.70-0.80
  good_improvement: 0.85+
  
task_app:
  type: classification
  input_field: patient_data
  output_field: diagnosis
  n_classes: 2
```

### gsm8k
```yaml
task_id: mipro-gsm8k
dataset:
  name: gsm8k
  source: huggingface
  hf_path: "gsm8k"
  n_train: 500
  n_eval: 100
  download_url: "https://synth-public-data.s3.amazonaws.com/gsm8k_train.jsonl"
  
evaluation:
  metric: exact_match
  seeds: [0, 1, 2, 3, 4]
  expected_baseline: 0.60-0.70
  good_improvement: 0.80+
  
task_app:
  type: math
  input_field: question
  output_field: answer
  requires_cot: true
```

### hover
```yaml
task_id: mipro-hover
dataset:
  name: hover
  source: langprobe
  n_train: 500
  n_eval: 100
  download_url: "https://synth-public-data.s3.amazonaws.com/hover_train.jsonl"
  
evaluation:
  metric: accuracy
  seeds: [0, 1, 2, 3, 4]
  expected_baseline: 0.55-0.65
  good_improvement: 0.75+
  
task_app:
  type: fact_verification
  input_field: claim
  context_field: evidence
  output_field: label
  n_classes: 2  # SUPPORTS, REFUTES
```

---

## Implementation Plan

### Phase 1: Infrastructure (Week 1)

1. **Add reproduction script requirement to agent instructions**
   - Update `instructions.py` to require `reproduce.py` artifact
   - Provide template in instructions
   - Validate script exists in artifact collection

2. **Add reproduction runner to test script**
   - Download `reproduce.py` from artifacts
   - Run it locally
   - Parse results and score

3. **Create task definition registry**
   - YAML/JSON files defining each benchmark task
   - Include dataset URLs, expected metrics, scoring params

### Phase 2: Core Tasks (Week 2)

4. **Upload datasets to public S3**
   - banking77_train.jsonl
   - iris_train.jsonl
   - heartdisease_train.jsonl

5. **Create example reproduction scripts**
   - Working `reproduce.py` for each Tier 1 task
   - Validate they run end-to-end

6. **Test agent on Tier 1 tasks**
   - mipro-banking77
   - gepa-banking77
   - mipro-iris
   - mipro-heartdisease

### Phase 3: LangProbe Tasks (Week 3)

7. **Upload LangProbe datasets**
   - gsm8k_train.jsonl
   - hover_train.jsonl
   - pupa_train.jsonl
   - hotpotqa_train.jsonl

8. **Create reference task apps for each**
   - Based on existing cookbooks examples
   - Ensure they work with reproduction script template

9. **Test agent on Tier 2 tasks**

### Phase 4: Scoring & Reporting (Week 4)

10. **Implement scoring system**
    - Per-task scores
    - Aggregate benchmark score
    - Leaderboard format

11. **Create benchmark report format**
    ```json
    {
      "benchmark_version": "v1.0",
      "agent_model": "gpt-5.1-codex-mini",
      "total_tasks": 8,
      "completed_tasks": 7,
      "aggregate_score": 78.5,
      "tasks": [
        {"id": "mipro-banking77", "score": 92, ...},
        ...
      ]
    }
    ```

12. **CI integration**
    - Run benchmark on schedule
    - Track regression

---

## File Locations

### Cookbooks Repo
```
cookbooks/
├── research_agent_benchmark.txt  # This file
├── benchmarks/
│   ├── tasks/                    # Task definitions
│   │   ├── mipro-banking77.yaml
│   │   ├── mipro-iris.yaml
│   │   ├── mipro-heartdisease.yaml
│   │   └── ...
│   ├── datasets/                 # Sample datasets (small, for testing)
│   │   ├── banking77_sample.jsonl
│   │   └── ...
│   └── reference/                # Reference implementations
│       ├── banking77/
│       │   ├── reproduce.py
│       │   ├── task_app.py
│       │   └── config.toml
│       └── ...
```

### Monorepo
```
monorepo/backend/
├── app/agent/research_runner/
│   ├── benchmarks/
│   │   ├── runner.py             # Runs benchmark suite
│   │   ├── scorer.py             # Scores results
│   │   └── reporter.py           # Generates reports
│   └── ...
└── scripts/
    └── run_research_benchmark.py # CLI for running benchmarks
```

---

## Success Criteria

### MVP (Phase 1-2)
- [ ] Agent produces `reproduce.py` for banking77 task
- [ ] Script runs locally with just API key
- [ ] Results match (within 5%) what agent reported

### v1.0 (All Phases)
- [ ] 8+ benchmark tasks defined
- [ ] All Tier 1 tasks working end-to-end
- [ ] Scoring system implemented
- [ ] CI runs benchmark weekly
- [ ] Benchmark score > 70% on all Tier 1 tasks

---

## Notes

- Start with MIPRO + Banking77 since we have it working
- Keep datasets small for fast iteration (100-500 samples)
- Use fixed seeds for reproducibility
- Store benchmark results in DB for tracking over time
- Consider different agent models (codex-mini vs max) as benchmark variants

